#! /bin/bash

source ./ceph-cluster-settings

function generate_conf {
	cat <<EOC > conf
public network = $NETWORK_IP.0/$NETWORK_MASK
cluster network = $NETWORK_IP.0/$NETWORK_MASK

#Choose reasonable numbers for number of replicas and placement groups.
osd pool default size = 2 # Write an object 2 times
osd pool default min size = 1 # Allow writing 1 copy in a degraded state
osd pool default pg num = 256
osd pool default pgp num = 256

#Choose a reasonable crush leaf type
#0 for a 1-node cluster.
#1 for a multi node cluster in a single rack
#2 for a multi node, multi chassis cluster with multiple hosts in a chassis
#3 for a multi node cluster with hosts across racks, etc.
osd crush chooseleaf type = 1
EOC

}

function generate_deploy_script {
	cat <<EOT > ceph-cluster-deploy
#! /bin/bash

ceph-deploy install $nodes
sleep 1

echo "AICIIIIIIIIIIIIIIIIIIIIII Pune si TU noile binare!!!"
read

mkdir ceph-deploy
cd ceph-deploy

cat ~/conf >> ceph.conf
cat ceph.conf
ceph-deploy new mon1 mon2 mon3
ceph-deploy mon create-initial
ceph-deploy gatherkeys mon1
sleep 1

ceph-deploy disk zap osd1:loop0 osd1:loop1 osd1:loop2
ceph-deploy osd create osd1:loop0:/dev/loop3p1 osd1:loop1:/dev/loop3p2 osd1:loop2:/dev/loop3p3
sleep 1

ceph-deploy disk zap osd2:loop4 osd2:loop5 osd2:loop6
ceph-deploy osd create osd2:loop4:/dev/loop7p1 osd2:loop5:/dev/loop7p2 osd2:loop6:/dev/loop7p3
sleep 1

ceph-deploy disk zap osd3:loop8 osd3:loop9 osd3:loop10
ceph-deploy osd create osd3:loop8:/dev/loop11p1 osd3:loop9:/dev/loop11p2 osd3:loop10:/dev/loop11p3
sleep 1

ceph-deploy admin $nodes
sudo chmod +r /etc/ceph/ceph.client.admin.keyring
sleep 1

ssh osd1 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3"
ssh osd2 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7"
ssh osd3 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11"

ssh osd1 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"
ssh osd2 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"
ssh osd3 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"

ssh mon1 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"
ssh mon2 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"
ssh mon3 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"

cat << EndOfHelp

*******************************************************
Ceph cluster created.
Now restart the cluster by running:
	./stop_cluster.sh && ./start_cluster.sh
on the host system in the docker-ceph-cluster dyrectory
*******************************************************

EndOfHelp
EOT

	chmod +x ceph-cluster-deploy
}

function ceph_services_stop {
	for node in $nodes; do
		ssh root@$node "sudo systemctl stop ceph-osd.target"
		ssh root@$node "sudo systemctl stop ceph-mon.target"
		ssh root@$node "sudo systemctl stop ceph"
	done
}

function ceph_services_start {
	for node in $nodes; do
		log $MODULE "start ceph on $node"
		ssh root@$node "sudo systemctl start ceph"

		log $MODULE "stop ceph-osd.target on $node"
		ssh root@$node "sudo systemctl stop ceph-osd.target"

		log $MODULE "start ceph-osd.target on $node"
		ssh root@$node "sudo systemctl start ceph-osd.target"

		log $MODULE "start ceph-mon.target on $node"
		ssh root@$node "sudo systemctl start ceph-mon.target"
	done
}

debs[1]="ceph-common_10.2.2-1_amd64.deb"
debs[2]="ceph-base_10.2.2-1_amd64.deb"
debs[3]="ceph-mon_10.2.2-1_amd64.deb"
debs[4]="ceph-mds_10.2.2-1_amd64.deb"
debs[5]="ceph_10.2.2-1_amd64.deb"
debs[6]="ceph-osd_10.2.2-1_amd64.deb"
debs[7]="python-rbd_10.2.2-1_amd64.deb"
debs[8]="python-cephfs_10.2.2-1_amd64.deb"
debs[9]="librados2_10.2.2-1_amd64.deb"
debs[10]="python-rados_10.2.2-1_amd64.deb"
debs[11]="librbd1_10.2.2-1_amd64.deb"
debs[12]="libcephfs1_10.2.2-1_amd64.deb"
debs[13]="librgw2_10.2.2-1_amd64.deb"
debs[14]="libradosstriper1_10.2.2-1_amd64.deb"

function copy_debs {
	MODULE="INSTALL_DEBS"
	DEB_PATH=$1

	for node in $nodes; do
	#node=admin
		log $MODULE "Copy .deb files from folder $DEB_PATH to node $node"
		ssh $node 'sudo rm -rf etc/apt/sources.list.d/ceph.list'
		ssh $node 'mkdir -p debs'
		for (( i = 1; i <= ${#debs[@]}; i++ )); do
			file="$DEB_PATH/${debs[$i]}"
			deb=${debs[$i]}
			pkg=$(echo $deb | cut -d'_' -f1)
			echo $pkg
			scp $file $node:./debs/
			ssh $node "sudo dpkg -i ~/debs/$deb"
			ssh $node "sudo apt-mark hold $pkg"
		done
		ssh $node "sudo apt-get update -y --fix-missing && sudo apt-get install -y -f && sudo apt-get update -y && sudo apt-get upgrade -y"
	done
}


function install_dependencies {
	MODULE="INSTALL__DEPENDENCIES"

	for node in $nodes; do
		log $MODULE "Copying install_deps.sh script..."
		scp deps install_deps.sh $node:.

		log $MODULE "Initiate install command..."
		ssh $node 'nohup ./install_deps.sh < /dev/null &'
	done
}
