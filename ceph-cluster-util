#! /bin/bash

source ./ceph-cluster-settings

function generate_conf {
	cat <<EOC > conf
public network = $NETWORK_IP.0/$NETWORK_MASK
cluster network = $NETWORK_IP.0/$NETWORK_MASK

#Choose reasonable numbers for number of replicas and placement groups.
osd pool default size = 2 # Write an object 2 times
osd pool default min size = 1 # Allow writing 1 copy in a degraded state
osd pool default pg num = 256
osd pool default pgp num = 256

#Choose a reasonable crush leaf type
#0 for a 1-node cluster.
#1 for a multi node cluster in a single rack
#2 for a multi node, multi chassis cluster with multiple hosts in a chassis
#3 for a multi node cluster with hosts across racks, etc.
osd crush chooseleaf type = 1
EOC

}

function generate_deploy_script {
	cat <<EOT > ceph-cluster-deploy
#! /bin/bash

ceph-deploy install $nodes
sleep 1

echo "AICIIIIIIIIIIIIIIIIIIIIII Pune si TU noile binare!!!"
read

mkdir ceph-deploy
cd ceph-deploy

cat ~/conf >> ceph.conf
cat ceph.conf
ceph-deploy new mon1 mon2 mon3
ceph-deploy mon create-initial
ceph-deploy gatherkeys mon1
sleep 1

ceph-deploy disk zap osd1:loop0 osd1:loop1 osd1:loop2
ceph-deploy osd create osd1:loop0:/dev/loop3p1 osd1:loop1:/dev/loop3p2 osd1:loop2:/dev/loop3p3
sleep 1

ceph-deploy disk zap osd2:loop4 osd2:loop5 osd2:loop6
ceph-deploy osd create osd2:loop4:/dev/loop7p1 osd2:loop5:/dev/loop7p2 osd2:loop6:/dev/loop7p3
sleep 1

ceph-deploy disk zap osd3:loop8 osd3:loop9 osd3:loop10
ceph-deploy osd create osd3:loop8:/dev/loop11p1 osd3:loop9:/dev/loop11p2 osd3:loop10:/dev/loop11p3
sleep 1

ceph-deploy admin $nodes
sudo chmod +r /etc/ceph/ceph.client.admin.keyring
sleep 1

ssh osd1 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop3"
ssh osd2 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop7"
ssh osd3 "sudo sgdisk -t 1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11 && sudo sgdisk -t 2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11 && sudo sgdisk -t 3:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/loop11"

ssh osd1 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"
ssh osd2 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"
ssh osd3 "sudo systemctl enable ceph-osd.target && sudo systemctl start ceph-osd.target && sudo systemctl status ceph-osd.target"

ssh mon1 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"
ssh mon2 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"
ssh mon3 "sudo systemctl enable ceph-mon.target && sudo systemctl start ceph-mon.target && sudo systemctl status ceph-mon.target"

cat << EndOfHelp

*******************************************************
Ceph cluster created.
Now restart the cluster by running:
	./stop_cluster.sh && ./start_cluster.sh
on the host system in the docker-ceph-cluster dyrectory
*******************************************************

EndOfHelp
EOT

	chmod +x ceph-cluster-deploy
}

function ceph_services_stop {
	for node in $nodes; do
		ssh root@$node "sudo systemctl stop ceph-osd.target"
		ssh root@$node "sudo systemctl stop ceph-mon.target"
		ssh root@$node "sudo systemctl stop ceph"
	done
}

function ceph_services_start {
	for node in $nodes; do
		ssh root@$node "sudo systemctl start ceph-osd.target"
		ssh root@$node "sudo systemctl start ceph-mon.target"
		ssh root@$node "sudo systemctl start ceph"
	done
}
